{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "# C = di default 1.0, rappresenta il peso degli errori: se voglio un soft margin, \n",
    "# dove gli errori contano poco uso C piccolo, se voglio un hard margin lo massimizzo\n",
    "# kernel = '' è il tipo di kernel che uso, di default è rbf, posso usare anche 'linear', 'poly', 'sigmoid', ...\n",
    "# degree = di default 3, serve solo per il polynomial kernel ed è la potenza a cui voglio elevare\n",
    "# gamma = 'auto' di default, è un valore che viene calcolato, meglio non toccare\n",
    "# coef0 = 0.0 di default, serve per alcuni kernel\n",
    "# probability = False di default, se true calcola la probabilità ovvero il livello di certezza della previsione \n",
    "# (simile a confidence di KNN), che è diverso\n",
    "# dall'accuratezza, ovvero quante ne predice in maniera esatta\n",
    "# shrinking = True di default, se è attivo, permette di non tenere in considerazione delle feature \n",
    "# che non hanno effetto sull'algoritmo, meglio lasciarlo attivo\n",
    "# tol = serve a modificare la tolleranza\n",
    "# cache size, serve a limitare la quantità di memoria, sarebbe la dimensione della cache del kernel, \n",
    "# quando crea i dati per la previsione\n",
    "# se il dataset è molto grande anche queato valore potrebbe diventare molto grande \n",
    "# ma si potrebbe incorrere in problemi se dopo averla limitata non ci sta\n",
    "# class_wight = , serve per modificare il peso delle classi se dovesse servire\n",
    "# verbose = non toccare\n",
    "# max_iter serve a settare un limite di iterazioni per trovare un margine che vada bene\n",
    "# perchè se imposto una tolleranza molto pretenziosa posso andare avanti molto tempo a cercare, serve a porre un limite\n",
    "# decision_function_shape = 'ovr, ovo, None', di default ora è ovr, sarebbe la tecnica che uso se l'algoritmo deve\n",
    "# decidere tra più di due classi\n",
    "# random_state serve per la probabilità, che però è abbastanza pesante soprattutto su grandi dataset\n",
    "\n",
    "# ATTRIBUTES\n",
    "# ci sono vari attributi che posso ottenere, come il nummero di support vectors, oppure w e b per la visualizzazione\n",
    "# e molti altri, meglio aprire la documentazione nel caso servano"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python38164bit164af38e2bcc47a2b21e3548f92575a3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
